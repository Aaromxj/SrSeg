# ------------------------------------------------------------------------------
# Copyright (c) Microsoft
# Licensed under the MIT License.
# Written by Ke Sun (sunk@mail.ustc.edu.cn)
# ------------------------------------------------------------------------------

import os

import cv2
import numpy as np
import random

import torch
from torch.nn import functional as F
from torch.utils import data

class BaseDataset(data.Dataset):
    def __init__(self, 
                 ignore_label=-1, 
                 base_size=2048, 
                 crop_size=(512, 1024), 
                 downsample_rate=1,
                 scale_factor=16,
                 #mean=[0.396506039426821, 0.425668084264726, 0.409969803748340, 0.411862257213112, 0.472768373018622, 0.479606246920914, 0.458538225323073, 0.439627072161108, 0.447395989292240, 0.440369144490999, 0.437383057336506, 0.426432996505487, 0.422464075569866, 0.409440134477110, 0.381587364395049, 0.343449976175233, 0.304237990914077, 0.277654885222196, 0.256373783077291, 0.230879218955624, 0.205328197052728, 0.182660015777012, 0.159913571205578, 0.140435062932334, 0.128934124984805, 0.123619791352061, 0.113820266495227, 0.105363770932795, 0.100056621393128, 0.0919704433403213, 0.0869858486635052, 0.0844577824136362, 0.0869840998989194, 0.0849841923365766, 0.0802528361760572, 0.0726565247999031, 0.0661119296069219, 0.0623773266999479, 0.0584310522272701, 0.0506222655904728, 0.0518492953900098, 0.0535646956411458, 0.0529021242249943, 0.0399642699067096, 0.0451114867470272, 0.0458124385755360, 0.0429326242927658, 0.0411709424256713, 0.0391285832813531, 0.0351451722820681, 0.0350590095385616, 0.0343399032463914, 0.0359498054465382, 0.0342178264167075, 0.0337208360740900, 0.0324928849306777, 0.0320399043678830, 0.0290062119441105, 0.0277023328250925, 0.0272374038046622, 0.0259280925146250, 0.0362807405768995, 0.0299537301546030, 0.0261334453720891, 0.0256997783889197, 0.0249686973980474, 0.0246497002304370, 0.0246138911109962, 0.0244440378669947, 0.0241565993050189, 0.0237252939088476, 0.0223231517529525, 0.0224901590408371, 0.0212830477907618, 0.0214609139325798, 0.0203113082216853, 0.0200439789504623, 0.0197792399666398, 0.0189669415600605, 0.0185217217577878, 0.0317371844178288, 0.0356182823928453, 0.0266271863234984, 0.0288362261232679, 0.0190846675662101, 0.0182773599085407, 0.0187244882827700, 0.0182712379677400, 0.0186681577410528, 0.0169147746504175, 0.0179924012444161, 0.0170685346467679, 0.0172465961345054, 0.0165931222698042, 0.0157155492962021, 0.0159659070206512, 0.0160668708321863, 0.0163022572056617, 0.0150327850730127, 0.0150286232631555, 0.0143531594993419, 0.0137378145128094, 0.0141107829851116, 0.0747778852863822, 0.0229387984527462, 0.0289018191873626, 0.254955898466651, 0.280728912895281, 0.300783588725783, 0.372507692084432, 0.286189994935178, 0.355127632847465, 0.315144948899569, 0.276489941952884, 0.257944361878524, 0.0706348558795929, 0.104775635461630, 0.0894575803642396, 0.0850546722643884, 0.0616795510600241, 0.0229077840784034, 0.0144874809239213, 0.0133350360120718, 0.0125607474231107, 0.0117642400634400, 0.0133199235346632, 0.0117513638046568, 0.0115665106067918, 0.0118154794089971, 0.0116012793232722, 0.0113499067835149, 0.0112793693873859, 0.0107857863710763, 0.0109159207456621, 0.0111636671487446, 0.0105180953509465, 0.0108576417059173, 0.0110101981832803, 0.0117358696384516, 0.0123075603568713, 0.0124762483273765, 0.0140028602388690, 0.0149119214668613, 0.0154954309990025, 0.0162545510564166, 0.0173371811718141, 0.0163106839122015, 0.0172090081437270, 0.0195964826488435, 0.0253588656775326, 0.0414111848105443, 0.150559914920616, 0.296841706794105, 0.333911976255226, 0.332874710374246, 0.338556944295865, 0.361312809570022, 0.327236647984573, 0.347559642186685, 0.339824175140503, 0.302942748981951, 0.317242859042527, 0.290296160751063, 0.290838204782225, 0.295669598567272, 0.300927511933281, 0.325983444035380, 0.288557039207714, 0.254332480773022, 0.222095570431305, 0.0826831572290381, 0.0412194500151445, 0.0350220265853358, 0.0415934509975310, 0.0781830663050513, 0.0413782939822660, 0.0214022521949206, 0.0158932969542586, 0.0184779640271752, 0.0192296882849554, 0.0178225070984441, 0.0146395731652344, 0.0130673268937905, 0.0132033437528784, 0.0124621677462590, 0.0122995835863976, 0.0126885467265276, 0.0122819905564852, 0.0133172605410591, 0.0138102177598890, 0.0148061671347140, 0.0135586708825712, 0.0130334003227300, 0.0130600008747191, 0.0129796869617349, 0.0143504020967698, 0.0142469298488914, 0.0144402861657031, 0.0157692549756769, 0.0176169958497399, 0.0206840449841805, 0.0239957335214987, 0.0267327380475530, 0.0349493884049220, 0.0425985054924485, 0.0449761153534916, 0.0340194118660951, 0.0424672507012703, 0.0663487993087826, 0.0465363480455467, 0.0562352773556595, 0.0723729940668594, 0.0781116374429178, 0.0713816749987591, 0.113207343063069, 0.141745210006498, 0.100163949872114, 0.155300868908961, 0.177508496621932, 0.189913245562611, 0.204303681909592, 0.271147551770658, 0.311710330405076, 0.287291429641275], 
                 #std=[0.00872192408230306, 0.00897318747030183, 0.00924361379419715, 0.0115552648025673, 0.0143030296640571, 0.0138264548837174, 0.0129065709986959, 0.0105695657429197, 0.00940165044124109, 0.00821118696025705, 0.00793416552149276, 0.00728515312379577, 0.00682357917118026, 0.00607777843428083, 0.00559379219965280, 0.00495978249406925, 0.00454642537832094, 0.00413317707724010, 0.00387011358956930, 0.00367114449136550, 0.00349709757067250, 0.00341555039393785, 0.00339242260809305, 0.00332949448680923, 0.00321543919140862, 0.00312332118974001, 0.00305034196175060, 0.00297800848028318, 0.00291705304277610, 0.00288010277301479, 0.00284238290868490, 0.00283403502482392, 0.00286613149462337, 0.00282584659490462, 0.00280272652978180, 0.00276836602319475, 0.00270851120794517, 0.00269404498458617, 0.00265713221817580, 0.00265206423359486, 0.00263983495938282, 0.00264817301675107, 0.00266183293987533, 0.00260210081739332, 0.00261659094440762, 0.00262796170386736, 0.00261670621097710, 0.00261773593663894, 0.00260617180211512, 0.00257242500168842, 0.00257222210842430, 0.00260346302092077, 0.00261968876291132, 0.00263186707117479, 0.00263939790857970, 0.00263449826046793, 0.00264265776777996, 0.00260854792385439, 0.00258660653756289, 0.00259848752861561, 0.00257250681761740, 0.00245777263634167, 0.00270481808063805, 0.00276241572563388, 0.00279538665225947, 0.00280462229953679, 0.00281832121584613, 0.00283496199814968, 0.00284988561126495, 0.00284553716871136, 0.00284268726642989, 0.00284512072542812, 0.00285612293411354, 0.00285362368010383, 0.00287841496939874, 0.00288194891588358, 0.00289030399312617, 0.00290136987563606, 0.00291256913001129, 0.00290791077784571, 0.00289962826181919, 0.00294864719442522, 0.00288686937639927, 0.00286489946398779, 0.00286180934189963, 0.00288202009923986, 0.00288581985035817, 0.00285525225091343, 0.00285735846724023, 0.00285634712176714, 0.00285181939407448, 0.00288730105394662, 0.00290775466806773, 0.00292215761030325, 0.00293039608110294, 0.00294664064090757, 0.00292885521665496, 0.00294169463890109, 0.00297957124878983, 0.00296626309250130, 0.00298163694541434, 0.00294449230696235, 0.00285996050105322, 0.00355834940451121, 0.00266675567886778, 0.00267921928280823, 0.00588447489217198, 0.00606911549685417, 0.00782981758291176, 0.00668276106412637, 0.00996432480961693, 0.00739040209170573, 0.00757435018990333, 0.00774631408821953, 0.00674385924003502, 0.00197255540530677, 0.00355194665286229, 0.00262530735790459, 0.00293162998363282, 0.00215744846493860, 0.00189372056421721, 0.00182156534761136, 0.00190140050133010, 0.00200629975959056, 0.00208152440092751, 0.00221519051771651, 0.00226365429027398, 0.00231644366590795, 0.00234970067361303, 0.00240476805620878, 0.00245661622379204, 0.00247921157874322, 0.00250105321309209, 0.00252669235629816, 0.00252736315635444, 0.00255178868111276, 0.00255029570015942, 0.00256061304234527, 0.00253881378937589, 0.00248923938147035, 0.00237653242981317, 0.00231412788219636, 0.00225510224183188, 0.00223310086521374, 0.00221245328030102, 0.00223920019211410, 0.00216329575498503, 0.00213470167382184, 0.00218773491651720, 0.00223640331061679, 0.00226989002796529, 0.00433945518542500, 0.00710170277481019, 0.00690393334951488, 0.00706140924204995, 0.00660488896759077, 0.00678703843683539, 0.00707131172307578, 0.00712027129463097, 0.00687236145914404, 0.00674545442151819, 0.00765626848475807, 0.00711480513211684, 0.00793429662706240, 0.00725403661333548, 0.00719421027141492, 0.00751898230831685, 0.00830593033354445, 0.00725364453262618, 0.00730197146407577, 0.00355504841982528, 0.00252479941995108, 0.00209740653164285, 0.00244460185905111, 0.00377490443346832, 0.00253238813251376, 0.00168603566967110, 0.00159679120483363, 0.00162354371518276, 0.00170905000405978, 0.00168765095264583, 0.00166454718778404, 0.00168842196068307, 0.00173040665168089, 0.00173108342642724, 0.00178471292327956, 0.00182647682098908, 0.00183469513001877, 0.00185249621822782, 0.00190747044569646, 0.00191224328154946, 0.00191508829976136, 0.00194380551802350, 0.00199713801234591, 0.00200170975568474, 0.00197847911653558, 0.00199520430688532, 0.00201444530936247, 0.00199982708007536, 0.00205570111199981, 0.00211661426558777, 0.00228060101636332, 0.00236860006970229, 0.00255112429367438, 0.00296349184726318, 0.00282149159574158, 0.00265636158844103, 0.00277098143885047, 0.00309281413093040, 0.00291354771486567, 0.00305218211813219, 0.00313063298713950, 0.00327613161096434, 0.00317756701723492, 0.00372569694199060, 0.00463819605619002, 0.00362945139070033, 0.00458366262905286, 0.00490023300572602, 0.00483696612638767, 0.00593381856151233, 0.00669380074177143, 0.00719393329475711, 0.00779992314081516]
                 mean=[0.485, 0.456, 0.406], 
                 std=[0.229, 0.224, 0.225]):

        self.base_size = base_size
        self.crop_size = crop_size
        self.ignore_label = ignore_label

        self.mean = mean
        self.std = std
        self.scale_factor = scale_factor
        self.downsample_rate = 1./downsample_rate

        self.files = []

    def __len__(self):
        return len(self.files)
    
    def input_transform(self, image):
        #print(image.shape)
        image = image.astype(np.float32)[:, :, ::-1]
        #image = image / 255.0
        #image -= self.mean
        #image /= self.std
        return image
    
    def label_transform(self, label):
        return np.array(label).astype('int32')

    def pad_image(self, image, h, w, size, padvalue):
        pad_image = image.copy()
        pad_h = max(size[0] - h, 0)
        pad_w = max(size[1] - w, 0)
        if pad_h > 0 or pad_w > 0:
            pad_image = cv2.copyMakeBorder(image, 0, pad_h, 0, 
                pad_w, cv2.BORDER_CONSTANT, 
                value=padvalue)
        
        return pad_image

    def rand_crop(self, image, label):
        h, w = image.shape[:-1]
        image = self.pad_image(image, h, w, self.crop_size,
                                (0.0, 0.0, 0.0))
        label = self.pad_image(label, h, w, self.crop_size,
                                (self.ignore_label,))
        
        new_h, new_w = label.shape
        x = random.randint(0, new_w - self.crop_size[1])
        y = random.randint(0, new_h - self.crop_size[0])
        image = image[y:y+self.crop_size[0], x:x+self.crop_size[1]]
        label = label[y:y+self.crop_size[0], x:x+self.crop_size[1]]

        return image, label

    def center_crop(self, image, label):
        h, w = image.shape[:2]
        x = int(round((w - self.crop_size[1]) / 2.))
        y = int(round((h - self.crop_size[0]) / 2.))
        image = image[y:y+self.crop_size[0], x:x+self.crop_size[1]]
        label = label[y:y+self.crop_size[0], x:x+self.crop_size[1]]

        return image, label
    
    def image_resize(self, image, long_size, label=None):
        h, w = image.shape[:2]
        if h > w:
            new_h = long_size
            new_w = np.int(w * long_size / h + 0.5)
        else:
            new_w = long_size
            new_h = np.int(h * long_size / w + 0.5)
        
        image = cv2.resize(image, (new_w, new_h), 
                           interpolation = cv2.INTER_LINEAR)
        if label is not None:
            label = cv2.resize(label, (new_w, new_h), 
                           interpolation = cv2.INTER_NEAREST)
        else:
            return image
        
        return image, label

    def multi_scale_aug(self, image, label=None, 
            rand_scale=1, rand_crop=True):
        long_size = np.int(self.base_size * rand_scale + 0.5)
        if label is not None:
            image, label = self.image_resize(image, long_size, label)
            if rand_crop:
                image, label = self.rand_crop(image, label)
            return image, label
        else:
            image = self.image_resize(image, long_size)
            return image

    def gen_sample(self, image, label, 
            multi_scale=True, is_flip=True, center_crop_test=False):
        if multi_scale:
            rand_scale = 0.5 + random.randint(0, self.scale_factor) / 10.0
            image, label = self.multi_scale_aug(image, label, 
                                                    rand_scale=rand_scale)

        if center_crop_test:
            image, label = self.image_resize(image, 
                                             self.base_size,
                                             label)
            image, label = self.center_crop(image, label)

        image = self.input_transform(image)
        label = self.label_transform(label)
        
        image = image.transpose((2, 0, 1))
        
        if is_flip:
            flip = np.random.choice(2) * 2 - 1
            image = image[:, :, ::flip]
            label = label[:, ::flip]

        if self.downsample_rate != 1:
            label = cv2.resize(label, 
                               None, 
                               fx=self.downsample_rate,
                               fy=self.downsample_rate, 
                               interpolation=cv2.INTER_NEAREST)

        return image, label

    def inference(self, model, image, flip=False):
        size = image.size()
        pred = model(image)
        pred = F.upsample(input=pred, 
                            size=(size[-2], size[-1]), 
                            mode='bilinear')        
        if flip:
            flip_img = image.numpy()[:,:,:,::-1]
            flip_output = model(torch.from_numpy(flip_img.copy()))
            flip_output = F.upsample(input=flip_output, 
                            size=(size[-2], size[-1]), 
                            mode='bilinear')
            flip_pred = flip_output.cpu().numpy().copy()
            flip_pred = torch.from_numpy(flip_pred[:,:,:,::-1].copy()).cuda()
            pred += flip_pred
            pred = pred * 0.5
        return pred.exp()

    def multi_scale_inference(self, model, image, scales=[1], flip=False):
        batch, _, ori_height, ori_width = image.size()
        assert batch == 1, "only supporting batchsize 1."
        device = torch.device("cuda:%d" % model.device_ids[0])
        image = image.numpy()[0].transpose((1,2,0)).copy()
        stride_h = np.int(self.crop_size[0] * 2.0 / 3.0)
        stride_w = np.int(self.crop_size[1] * 2.0 / 3.0)
        final_pred = torch.zeros([1, self.num_classes,
                                    ori_height,ori_width]).to(device)
        padvalue = -1.0  * np.array(self.mean) / np.array(self.std)
        for scale in scales:
            new_img = self.multi_scale_aug(image=image,
                                           rand_scale=scale,
                                           rand_crop=False)
            height, width = new_img.shape[:-1]
                
            if max(height, width) <= np.min(self.crop_size):
                new_img = self.pad_image(new_img, height, width, 
                                    self.crop_size, padvalue)
                new_img = new_img.transpose((2, 0, 1))
                new_img = np.expand_dims(new_img, axis=0)
                new_img = torch.from_numpy(new_img)
                preds = self.inference(model, new_img, flip)
                preds = preds[:, :, 0:height, 0:width]
            else:
                if height < self.crop_size[0] or width < self.crop_size[1]:
                    new_img = self.pad_image(new_img, height, width, 
                                        self.crop_size, padvalue)
                new_h, new_w = new_img.shape[:-1]
                rows = np.int(np.ceil(1.0 * (new_h - 
                                self.crop_size[0]) / stride_h)) + 1
                cols = np.int(np.ceil(1.0 * (new_w - 
                                self.crop_size[1]) / stride_w)) + 1
                preds = torch.zeros([1, self.num_classes,
                                           new_h,new_w]).to(device)
                count = torch.zeros([1,1, new_h, new_w]).to(device)

                for r in range(rows):
                    for c in range(cols):
                        h0 = r * stride_h
                        w0 = c * stride_w
                        h1 = min(h0 + self.crop_size[0], new_h)
                        w1 = min(w0 + self.crop_size[1], new_w)
                        crop_img = new_img[h0:h1, w0:w1, :]
                        if h1 == new_h or w1 == new_w:
                            crop_img = self.pad_image(crop_img, 
                                                      h1-h0, 
                                                      w1-w0, 
                                                      self.crop_size, 
                                                      padvalue)
                        crop_img = crop_img.transpose((2, 0, 1))
                        crop_img = np.expand_dims(crop_img, axis=0)
                        crop_img = torch.from_numpy(crop_img)
                        pred = self.inference(model, crop_img, flip)

                        preds[:,:,h0:h1,w0:w1] += pred[:,:, 0:h1-h0, 0:w1-w0]
                        count[:,:,h0:h1,w0:w1] += 1
                preds = preds / count
                preds = preds[:,:,:height,:width]
            preds = F.upsample(preds, (ori_height, ori_width), 
                                   mode='bilinear')
            final_pred += preds
        return final_pred
